{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "w6kFjHWA7BS5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class conv_attention2d(nn.Module):\n",
        "  \n",
        "  def __init__(self, in_dim, out_dim, kernel_size=3, stride=1, padding=0):\n",
        "    super(conv_attention2d, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "    self.w_q = nn.Parameter(torch.ones((in_dim, out_dim)))\n",
        "    self.w_k = nn.Parameter(torch.ones((in_dim, out_dim)))\n",
        "    self.w_v = nn.Parameter(torch.ones((in_dim, out_dim)))\n",
        "    self.padding = nn.ZeroPad2d(padding)\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.padding(x)\n",
        "    # x.shape = (h, w, c)\n",
        "    x = x.permute(1,2,0)\n",
        "    print(x.shape)\n",
        "    # x_q.shape = x_k.shape = x_v.shape = (h, w, c)\n",
        "    \n",
        "    x_q = x @ self.w_q\n",
        "    x_k = x @ self.w_k\n",
        "    x_v = x @ self.w_v\n",
        "\n",
        "    output_size = (x.shape[0]-self.kernel_size)//self.stride + 1\n",
        "    ans = torch.zeros(output_size, output_size, self.out_dim)\n",
        "\n",
        "    for h in range(0, x.shape[0] - self.kernel_size +1, self.stride):\n",
        "      for w in range(0, x.shape[1] - self.kernel_size +1, self.stride):\n",
        "        # q.shape = (1, 1, c)\n",
        "        q = x_q[h+self.kernel_size//2, w+self.kernel_size//2, :]\n",
        "        # q.shape = (c)\n",
        "        q = q.squeeze()\n",
        "\n",
        "        # kT.shape = (c, ks*ks) Tranpose\n",
        "        kT = x_k[h:h+self.kernel_size, w:w+self.kernel_size, :].flatten(0,1).permute(1, 0)\n",
        "\n",
        "        # qkT.shape = (ks*ks)\n",
        "        qkT = q @ kT\n",
        "        qkT /= sqrt(self.out_dim)\n",
        "        qkT = F.softmax(qkT)\n",
        "\n",
        "        # v.shape = (ks*ks, c)\n",
        "        v = x_k[h:h+self.kernel_size, w:w+self.kernel_size, :].flatten(0,1)\n",
        "        qkTv = qkT @ v\n",
        "\n",
        "        ans[h//self.stride, w//self.stride, :] = qkTv\n",
        "        # ans[h, w, :] = x[h, w, :]\n",
        "\n",
        "    return ans.permute(2,0,1)\n"
      ],
      "metadata": {
        "id": "MbENHBSl7OPk"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = torch.randn((3, 7, 7))\n",
        "print(img.shape)\n",
        "\n",
        "ca = conv_attention2d(img.shape[0], 768, kernel_size = 3, padding=1, stride=1)\n",
        "\n",
        "\n",
        "print(ca(img).shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9uWVMER8Ump",
        "outputId": "e0f91d05-d97b-468a-a10e-c2fcac6c719a"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 7, 7])\n",
            "torch.Size([9, 9, 3])\n",
            "torch.Size([768, 7, 7])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-174-77d89a33554f>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  qkT = F.softmax(qkT)\n"
          ]
        }
      ]
    }
  ]
}